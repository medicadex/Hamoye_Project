{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO4PCwC3CmjBpGIgaqdvlCI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/medicadex/Hamoye_Project/blob/main/Adebowale_qazeem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yd6yALvig0y",
        "outputId": "44b60daa-7f32-4e40-81b0-25debfe47a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['customerID' 'gender' 'SeniorCitizen' 'Partner' 'Dependents' 'tenure'\n",
            " 'PhoneService' 'MultipleLines' 'InternetService' 'OnlineSecurity'\n",
            " 'OnlineBackup' 'DeviceProtection' 'TechSupport' 'StreamingTV'\n",
            " 'StreamingMovies' 'Contract' 'PaperlessBilling' 'PaymentMethod'\n",
            " 'MonthlyCharges' 'TotalCharges' 'Churn']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df= pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "print(np.array(df.columns))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert 'TotalCharges' to numeric and fill missing values with 0\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce').fillna(0)\n",
        "\n",
        "# Convert 'Churn' column to binary\n",
        "df['Churn'] = df['Churn'].map({'No': 0, 'Yes': 1})\n",
        "\n",
        "# Define features\n",
        "categorical = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',\n",
        "               'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
        "               'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
        "numerical = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X = df[categorical + numerical]\n",
        "y = df['Churn']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n"
      ],
      "metadata": {
        "id": "LI03jweOt7Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define preprocessing steps\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(sparse=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numerical),\n",
        "        ('cat', categorical_transformer, categorical)\n",
        "    ])\n",
        "\n",
        "# Fit and transform the preprocessing pipeline on the training data\n",
        "X_train_prep = preprocessor.fit_transform(X_train)\n",
        "X_test_prep = preprocessor.transform(X_test)\n",
        "\n",
        "# Get column names for one-hot encoded features\n",
        "ohe_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical)\n",
        "\n",
        "# Combine scaled numerical and one-hot encoded categorical features into train and test set dataframes\n",
        "X_train_prep_df = pd.DataFrame(X_train_prep, columns=numerical + list(ohe_feature_names))\n",
        "X_test_prep_df = pd.DataFrame(X_test_prep, columns=numerical + list(ohe_feature_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "014L5MQjt9Vb",
        "outputId": "fddcd97f-a5a7-43ec-ad53-9e22919fbde1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# Train models\n",
        "rf = RandomForestClassifier(random_state=1)\n",
        "et = ExtraTreesClassifier(random_state=1)\n",
        "xgb = XGBClassifier(random_state=1)\n",
        "lgbm = LGBMClassifier(random_state=1)\n",
        "\n",
        "rf.fit(X_train_prep_df, y_train)\n",
        "et.fit(X_train_prep_df, y_train)\n",
        "xgb.fit(X_train_prep_df, y_train)\n",
        "lgbm.fit(X_train_prep_df, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "rf_score = rf.score(X_test_prep_df, y_test)\n",
        "et_score = et.score(X_test_prep_df, y_test)\n",
        "xgb_score = xgb.score(X_test_prep_df, y_test)\n",
        "lgbm_score = lgbm.score(X_test_prep_df, y_test)\n",
        "\n",
        "print(\"Random Forest Accuracy:\", rf_score)\n",
        "print(\"Extra Trees Accuracy:\", et_score)\n",
        "print(\"XGBoost Accuracy:\", xgb_score)\n",
        "print(\"LightGBM Accuracy:\", lgbm_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrKN3ziLuDsK",
        "outputId": "41e7a313-bea5-4f35-9e59-95ef53020a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1521, number of negative: 4113\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001561 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 670\n",
            "[LightGBM] [Info] Number of data points in the train set: 5634, number of used features: 46\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.269968 -> initscore=-0.994785\n",
            "[LightGBM] [Info] Start training from score -0.994785\n",
            "Random Forest Accuracy: 0.7913413768630234\n",
            "Extra Trees Accuracy: 0.7672107877927609\n",
            "XGBoost Accuracy: 0.7934705464868701\n",
            "LightGBM Accuracy: 0.8034066713981547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "# Define hyperparameter grid\n",
        "n_estimators = [50, 100, 300, 500, 1000]\n",
        "min_samples_split = [2, 3, 5, 7, 9]\n",
        "min_samples_leaf = [1, 2, 4, 6, 8]\n",
        "max_features = ['auto', 'sqrt', 'log2', None]\n",
        "\n",
        "hyperparameter_grid = {\n",
        "    'n_estimators': n_estimators,\n",
        "    'min_samples_leaf': min_samples_leaf,\n",
        "    'min_samples_split': min_samples_split,\n",
        "    'max_features': max_features\n",
        "}\n",
        "\n",
        "# Instantiate ExtraTreesClassifier\n",
        "etc = ExtraTreesClassifier(random_state=1)\n",
        "\n",
        "# Set up RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(etc, hyperparameter_grid, cv=5, n_iter=10, scoring='accuracy', n_jobs=-1, verbose=1, random_state=1)\n",
        "\n",
        "# Fit RandomizedSearchCV on training data\n",
        "random_search.fit(X_train_prep_df, y_train)\n",
        "\n",
        "# Retrieve the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv9_mzIOvI4X",
        "outputId": "f5b1055a-c864-451b-d00d-e318b40889e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best Hyperparameters: {'n_estimators': 1000, 'min_samples_split': 9, 'min_samples_leaf': 8, 'max_features': 'sqrt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "# Instantiate a new ExtraTreesClassifier with the best hyperparameters\n",
        "best_etc = ExtraTreesClassifier(random_state=1, n_estimators=1000, min_samples_split=9, min_samples_leaf=8, max_features='sqrt')\n",
        "\n",
        "# Fit the new model on the training data\n",
        "best_etc.fit(X_train_prep_df, y_train)\n",
        "\n",
        "# Evaluate the accuracy of the new model on the test set\n",
        "best_etc_score = best_etc.score(X_test_prep_df, y_test)\n",
        "\n",
        "# Compare the accuracy of the new model with the initial model\n",
        "print(\"Initial ExtraTreesClassifier Accuracy:\", et_score)\n",
        "print(\"Optimal ExtraTreesClassifier Accuracy:\", best_etc_score)\n",
        "\n",
        "# Check if the accuracy of the new optimal model is higher or lower than the initial model\n",
        "if best_etc_score > et_score:\n",
        "    print(\"The accuracy of the new optimal model is higher.\")\n",
        "elif best_etc_score < et_score:\n",
        "    print(\"The accuracy of the new optimal model is lower.\")\n",
        "else:\n",
        "    print(\"The accuracy of the new optimal model is the same as the initial model.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VM_fMKByRA3",
        "outputId": "3fee8a8a-9fce-46a4-dcae-95209ea65195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial ExtraTreesClassifier Accuracy: 0.7672107877927609\n",
            "Optimal ExtraTreesClassifier Accuracy: 0.8041163946061036\n",
            "The accuracy of the new optimal model is higher.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importances from the optimal ExtraTreesClassifier model\n",
        "feature_importances = best_etc.feature_importances_\n",
        "\n",
        "# Create a DataFrame to display feature importances alongside feature names\n",
        "feature_importance_df = pd.DataFrame({'Feature': X_train_prep_df.columns, 'Importance': feature_importances})\n",
        "\n",
        "# Sort feature importances in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the two most important features\n",
        "print(\"Two most important features:\")\n",
        "print(feature_importance_df.head(2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5K9W7P6yRl_",
        "outputId": "27ca9e01-b5e3-4729-ce1d-19bf3bb9de5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Two most important features:\n",
            "                    Feature  Importance\n",
            "37  Contract_Month-to-month    0.152237\n",
            "0                    tenure    0.092800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GQgbGcc7zTyk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}